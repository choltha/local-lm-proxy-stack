model_list:
  - model_name: phi3
    litellm_params:
      model:  ollama/phi3:3.8b-mini-128k-instruct-q6_K
      api_base: http://host.docker.internal:11434

litellm_settings:
  success_callback: ["langfuse"]
  set_verbose: True
  # disable if multiple calls with same temperature should not be cached or send additional flag in call to avoid caching
  cache: True